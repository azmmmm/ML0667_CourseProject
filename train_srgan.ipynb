{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.backends.cudnn as cudnn\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from lib.models import Generator, Discriminator, TruncatedVGG19\n",
    "from lib.dataloaders import SRDataset\n",
    "from lib.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集参数\n",
    "data_folder = './data/'    # 数据存放路径\n",
    "crop_size = 96             # 高分辨率图像裁剪尺寸\n",
    "scaling_factor = 4         # 放大比例\n",
    "\n",
    "# 生成器模型参数(与SRResNet相同)\n",
    "large_kernel_size_g = 9   # 第一层卷积和最后一层卷积的核大小\n",
    "small_kernel_size_g = 3   # 中间层卷积的核大小\n",
    "n_channels_g = 64         # 中间层通道数\n",
    "n_blocks_g = 16           # 残差模块数量\n",
    "srresnet_checkpoint = \"./results/srresnet.pth\"  # 预训练的SRResNet模型，用来初始化\n",
    "\n",
    "# 判别器模型参数\n",
    "kernel_size_d = 3  # 所有卷积模块的核大小\n",
    "n_channels_d = 64  # 第1层卷积模块的通道数, 后续每隔1个模块通道数翻倍\n",
    "n_blocks_d = 8     # 卷积模块数量\n",
    "fc_size_d = 1024  # 全连接层连接数\n",
    "\n",
    "# 学习参数\n",
    "batch_size = 40    # 批大小 批大小过小可能使GAN难以收敛\n",
    "start_epoch = 1     # 迭代起始位置\n",
    "epochs = 10         # 迭代轮数\n",
    "checkpoint = None   # SRGAN预训练模型, 如果没有则填None\n",
    "workers = 4         # 加载数据线程数量\n",
    "vgg19_i = 5         # VGG19网络第i个池化层\n",
    "vgg19_j = 4         # VGG19网络第j个卷积层\n",
    "beta = 1e-4         # 判别损失乘子\n",
    "g_lr = 1e-4           # 生成器学习率\n",
    "d_lr = g_lr*2         # 判别器学习率\n",
    "\n",
    "#sample\n",
    "is_sample=False\n",
    "sample_batchs=1000//batch_size\n",
    "# 设备参数\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ngpu = 1                # 用来运行的gpu数量\n",
    "cudnn.benchmark = True   # 对卷积进行加速\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(writer,model_name):\n",
    "    global checkpoint,start_epoch\n",
    "\n",
    "    # 模型初始化\n",
    "    generator = Generator(large_kernel_size=large_kernel_size_g,\n",
    "                              small_kernel_size=small_kernel_size_g,\n",
    "                              n_channels=n_channels_g,\n",
    "                              n_blocks=n_blocks_g,\n",
    "                              scaling_factor=scaling_factor)\n",
    "\n",
    "    discriminator = Discriminator(kernel_size=kernel_size_d,\n",
    "                                    n_channels=n_channels_d,\n",
    "                                    n_blocks=n_blocks_d,\n",
    "                                    fc_size=fc_size_d)\n",
    "\n",
    "    # 初始化优化器\n",
    "    optimizer_g = torch.optim.Adam(params=filter(lambda p: p.requires_grad,generator.parameters()),lr=g_lr)\n",
    "    optimizer_d = torch.optim.Adam(params=filter(lambda p: p.requires_grad,discriminator.parameters()),lr=d_lr)\n",
    "\n",
    "    # 截断的VGG19网络用于计算损失函数\n",
    "    truncated_vgg19 = TruncatedVGG19(i=vgg19_i, j=vgg19_j)\n",
    "    truncated_vgg19.eval()\n",
    "\n",
    "    # 损失函数\n",
    "    content_loss_criterion = nn.MSELoss()\n",
    "    adversarial_loss_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # 将数据移至默认设备\n",
    "    generator = generator.to(device)\n",
    "    discriminator = discriminator.to(device)\n",
    "    truncated_vgg19 = truncated_vgg19.to(device)\n",
    "    content_loss_criterion = content_loss_criterion.to(device)\n",
    "    adversarial_loss_criterion = adversarial_loss_criterion.to(device)\n",
    "    \n",
    "    # 加载预训练模型\n",
    "    srresnetcheckpoint = torch.load(srresnet_checkpoint)\n",
    "    generator.net.load_state_dict(srresnetcheckpoint['model'])\n",
    "\n",
    "    if checkpoint is not None:\n",
    "        checkpoint = torch.load(checkpoint)\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        generator.load_state_dict(checkpoint['generator'])\n",
    "        discriminator.load_state_dict(checkpoint['discriminator'])\n",
    "        optimizer_g.load_state_dict(checkpoint['optimizer_g'])\n",
    "        optimizer_d.load_state_dict(checkpoint['optimizer_d'])\n",
    "    \n",
    "    # 单机多GPU训练\n",
    "    if torch.cuda.is_available() and ngpu > 1:\n",
    "        generator = nn.DataParallel(generator, device_ids=list(range(ngpu)))\n",
    "        discriminator = nn.DataParallel(discriminator, device_ids=list(range(ngpu)))\n",
    "\n",
    "    # 定制化的dataloaders\n",
    "    train_dataset = SRDataset(data_folder,split='train',\n",
    "                              crop_size=crop_size,\n",
    "                              scaling_factor=scaling_factor,\n",
    "                              lr_img_type='imagenet-norm',\n",
    "                              hr_img_type='imagenet-norm')\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=workers,\n",
    "        pin_memory=True) \n",
    "\n",
    "    # 开始逐轮训练\n",
    "    for epoch in range(start_epoch, epochs+1):\n",
    "        \n",
    "        if epoch == int(epochs / 2):  # 执行到一半时降低学习率\n",
    "            adjust_learning_rate(optimizer_g, 0.1)\n",
    "            adjust_learning_rate(optimizer_d, 0.1)\n",
    "\n",
    "        generator.train()   # 开启训练模式：允许使用批样本归一化\n",
    "        discriminator.train()\n",
    "\n",
    "        losses_c = AverageMeter()  # 内容损失\n",
    "        losses_a = AverageMeter()  # 生成损失\n",
    "        losses_d = AverageMeter()  # 判别损失\n",
    "\n",
    "        n_iter = len(train_loader)\n",
    "\n",
    "        # 按批处理\n",
    "        for i, (lr_imgs, hr_imgs) in enumerate(train_loader):\n",
    "\n",
    "            # 数据移至默认设备进行训练\n",
    "            lr_imgs = lr_imgs.to(device)  # (batch_size (N), 3, 24, 24),  imagenet-normed 格式\n",
    "            hr_imgs = hr_imgs.to(device)  # (batch_size (N), 3, 96, 96),  imagenet-normed 格式\n",
    "\n",
    "            #-----------------------1. 生成器更新----------------------------\n",
    "            # 生成\n",
    "            sr_imgs = generator(lr_imgs)  # (N, 3, 96, 96), 范围在 [-1, 1]\n",
    "            sr_imgs = convert_image(\n",
    "                sr_imgs, source='[-1, 1]',\n",
    "                target='imagenet-norm')  # (N, 3, 96, 96), imagenet-normed\n",
    "\n",
    "            # 计算 VGG 特征图\n",
    "            sr_imgs_in_vgg_space = truncated_vgg19(sr_imgs)              # batchsize X 512 X 6 X 6\n",
    "            hr_imgs_in_vgg_space = truncated_vgg19(hr_imgs).detach()     # batchsize X 512 X 6 X 6\n",
    "\n",
    "            # 计算内容损失\n",
    "            content_loss = content_loss_criterion(sr_imgs_in_vgg_space,hr_imgs_in_vgg_space)\n",
    "\n",
    "            # 计算生成损失\n",
    "            sr_discriminated = discriminator(sr_imgs)  # (batch X 1)   \n",
    "            adversarial_loss = adversarial_loss_criterion(\n",
    "                sr_discriminated, torch.ones_like(sr_discriminated)) # 生成器希望生成的图像能够完全迷惑判别器，因此它的预期所有图片真值为1\n",
    "\n",
    "            # 计算总的感知损失\n",
    "            perceptual_loss = content_loss + beta * adversarial_loss\n",
    "\n",
    "            # 后向传播.\n",
    "            optimizer_g.zero_grad()\n",
    "            perceptual_loss.backward()\n",
    "\n",
    "            # 更新生成器参数\n",
    "            optimizer_g.step()\n",
    "\n",
    "            #记录损失值\n",
    "            losses_c.update(content_loss.item(), lr_imgs.size(0))\n",
    "            losses_a.update(adversarial_loss.item(), lr_imgs.size(0))\n",
    "\n",
    "            #-----------------------2. 判别器更新----------------------------\n",
    "            # 判别器判断\n",
    "            hr_discriminated = discriminator(hr_imgs)\n",
    "            sr_discriminated = discriminator(sr_imgs.detach())\n",
    "\n",
    "            # 二值交叉熵损失\n",
    "            adversarial_loss = adversarial_loss_criterion(sr_discriminated, torch.zeros_like(sr_discriminated)) + \\\n",
    "                            adversarial_loss_criterion(hr_discriminated, torch.ones_like(hr_discriminated))  # 判别器希望能够准确的判断真假，因此凡是生成器生成的都设置为0，原始图像均设置为1\n",
    "\n",
    "            # 后向传播\n",
    "            optimizer_d.zero_grad()\n",
    "            adversarial_loss.backward()\n",
    "\n",
    "            # 更新判别器\n",
    "            optimizer_d.step()\n",
    "\n",
    "            # 记录损失\n",
    "            losses_d.update(adversarial_loss.item(), hr_imgs.size(0))\n",
    "\n",
    "            # 监控图像变化\n",
    "            if (is_sample and i==sample_batchs-2) or (i==(n_iter-2)):\n",
    "                writer.add_image('SRGAN/epoch_'+str(epoch)+'_1_lr', make_grid(lr_imgs[:4,:3,:,:].cpu(), nrow=4, normalize=True),epoch)\n",
    "                writer.add_image('SRGAN/epoch_'+str(epoch)+'_2_sr', make_grid(sr_imgs[:4,:3,:,:].cpu(), nrow=4, normalize=True),epoch)\n",
    "                writer.add_image('SRGAN/epoch_'+str(epoch)+'_3_hr', make_grid(hr_imgs[:4,:3,:,:].cpu(), nrow=4, normalize=True),epoch)\n",
    "\n",
    "            # 打印结果\n",
    "            print(\"第\"+str(epoch)+\"/\"+str(epochs)+\"个epoch,第\"+str(i)+\"/\"+str(sample_batchs if is_sample else len(train_loader))+\" 个batch训练结束    \",end='\\r')\n",
    "\n",
    "            \n",
    "\n",
    "            #sample\n",
    "            if(is_sample and i>= sample_batchs):\n",
    "                break\n",
    "\n",
    "        # 手动释放内存              \n",
    "            del lr_imgs, hr_imgs, sr_imgs, hr_imgs_in_vgg_space, sr_imgs_in_vgg_space, hr_discriminated, sr_discriminated  # 手工清除掉缓存\n",
    "        \n",
    "\n",
    "        # 监控损失值变化\n",
    "        writer.add_scalar('SRGAN/Loss_c', losses_c.val, epoch) \n",
    "        writer.add_scalar('SRGAN/Loss_a', losses_a.val, epoch)    \n",
    "        writer.add_scalar('SRGAN/Loss_d', losses_d.val, epoch)    \n",
    "\n",
    "        # 保存预训练模型\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'generator': generator.state_dict(),\n",
    "            'discriminator': discriminator.state_dict(),\n",
    "            'optimizer_g': optimizer_g.state_dict(),\n",
    "            'optimizer_d': optimizer_d.state_dict(),\n",
    "        }, 'results/'+model_name)\n",
    "    \n",
    "    # 训练结束关闭监控"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第49/100个epoch,第25/25 个batch训练结束    \n",
      "调整学习率.\n",
      "新的学习率为 0.000010\n",
      "\n",
      "\n",
      "调整学习率.\n",
      "新的学习率为 0.000020\n",
      "\n",
      "第100/100个epoch,第25/25 个batch训练结束    \r"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(filename_suffix=\"_gan_sample\") # 实时监控     使用命令 tensorboard --logdir runs  进行查看\n",
    "epochs=100\n",
    "is_sample=True\n",
    "model_name='srgan_sample.pth'\n",
    "train(writer,model_name)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
